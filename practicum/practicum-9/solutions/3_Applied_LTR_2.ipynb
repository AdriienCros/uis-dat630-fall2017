{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied LTR, Part II.\n",
    "\n",
    "Building on Part I of this exercise, expand it as follows.\n",
    "\n",
    "1. **Apply min-max normalization on the feature values** and see if it changes retrieval performance. Note that you need to use the same folds for cross-validation, in order to make it a fair comparison.\n",
    "  - Min-max normalization: $\\tilde{x}_i = \\frac{x_i -\\min(x)}{\\max(x) - \\min(x)}$\n",
    "    - $x_1,\\dots,x_n$ are the original values for a given feature\n",
    "    - $\\tilde{x}_i$ is the transformed feature value for the $i$th instance\n",
    "2. **Add query and document features to your feature vector** and evaluate performance. \n",
    "  - Example query features\n",
    "    - The length of the query, i.e., number of terms\n",
    "    - Avg. IDF score of query terms\n",
    "  - Example document features\n",
    "    - The length of each field (title, content), i.e., number of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "INDEX_NAME = \"aquaint\"\n",
    "DOC_TYPE = \"doc\"\n",
    "\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_FILE = \"data/queries.txt\"  # make sure the query file exists on this location\n",
    "QRELS_FILE = \"data/qrels2.csv\"  # file with the relevance judgments (ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATURES_FILE = \"data/features.txt\"  # output the features in this file\n",
    "OUTPUT_FILE = \"data/ltr.txt\"  # output the ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries\n",
    "\n",
    "queries = load_queries(QUERY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_qrels(qrels_file):\n",
    "    gt = {}  # holds a list of relevant documents for each queryID\n",
    "    with open(qrels_file, \"r\") as fin:\n",
    "        header = fin.readline().strip()\n",
    "        if header != \"queryID,docIDs\":\n",
    "            raise Exception(\"Incorrect file format!\")\n",
    "        for line in fin.readlines():\n",
    "            qid, docids = line.strip().split(\",\")\n",
    "            gt[qid] = docids.split()\n",
    "    return gt\n",
    "            \n",
    "qrels = load_qrels(QRELS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Creating training data and writing it to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features for query-document pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 6 features in total. Each feature here is a retrieval score, which we obtain using a different ES configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ES_CONFIG = {\n",
    "    1: {\n",
    "        \"field\": \"content\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"BM25\", \n",
    "                \"b\": 0.75, \n",
    "                \"k1\": 1.2\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    2: {\n",
    "        \"field\": \"title\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"BM25\", \n",
    "                \"b\": 0.75, \n",
    "                \"k1\": 1.2\n",
    "            } \n",
    "        }\n",
    "    },    \n",
    "    3: {\n",
    "        \"field\": \"content\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"LMDirichlet\", \n",
    "                \"mu\": 2000  # larger for content\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    4: {\n",
    "        \"field\": \"title\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"LMDirichlet\", \n",
    "                \"mu\": 200  # small for title\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    5: {\n",
    "        \"field\": \"content\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"LMJelinekMercer\", \n",
    "                \"lambda\": 0.1  \n",
    "            } \n",
    "        }\n",
    "    },    \n",
    "    6: {\n",
    "        \"field\": \"title\",\n",
    "        \"similarity\": {\n",
    "            \"default\": {\n",
    "                \"type\": \"LMJelinekMercer\", \n",
    "                \"lambda\": 0.1  \n",
    "            } \n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "NUM_FEAT = len(ES_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-max feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minmax_norm(features, fid):\n",
    "    \"\"\"Normalizes a given feature.\"\"\"\n",
    "    # this is to be done for each query separately\n",
    "    for qid, fts in features.items():\n",
    "        min_x = 10000 # sufficiently large number\n",
    "        max_x = -10000 # # sufficiently small number\n",
    "        for docid in fts.keys():\n",
    "            x = features[qid][docid][fid]\n",
    "            if x < min_x:\n",
    "                min_x = x\n",
    "            if x > max_x:\n",
    "                max_x = x\n",
    "        for docid in fts.keys():\n",
    "            x = features[qid][docid][fid]\n",
    "            features[qid][docid][fid] = (x - min_x) / (max_x - min_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting feature values in the `features` dict. It has the structure `features[qid][docid][fid] = value`, where fid is a feature ID (1..6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query-document features\n",
    "\n",
    "  * The first feature in our set will be the BM25 content retrieval score. This is special in that this is the **candidate document set** we will rerank. We only consider the top-100 documents here.\n",
    "  * For other field/model combinations, we retrieve the top-1000 documents. However, we only keep those that are in the candidate document set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing values for feature # 1\n",
      "Computing values for feature # 2\n",
      "Computing values for feature # 3\n",
      "Computing values for feature # 4\n",
      "Computing values for feature # 5\n",
      "Computing values for feature # 6\n"
     ]
    }
   ],
   "source": [
    "for fid in range(1, len(ES_CONFIG) + 1):\n",
    "    print(\"Computing values for feature #\", fid)\n",
    "    # Set ES similarity config\n",
    "    es.indices.close(index=INDEX_NAME)\n",
    "    es.indices.put_settings(index=INDEX_NAME, body={\"similarity\": ES_CONFIG[fid][\"similarity\"]})\n",
    "    es.indices.open(index=INDEX_NAME)\n",
    "\n",
    "    time.sleep(1)  # wait until it takes effect\n",
    "\n",
    "    for qid, query in queries.items():\n",
    "        if qid not in features:\n",
    "            features[qid] = {}\n",
    "        num_docs = 100 if fid == 1 else 1000\n",
    "        res = es.search(index=INDEX_NAME, q=query, df=ES_CONFIG[fid][\"field\"], _source=False, size=num_docs).get('hits', {})\n",
    "        for doc in res.get(\"hits\", {}):\n",
    "            docid = doc.get(\"_id\")\n",
    "            if fid == 1:  # for BM25 content, we keep all docs; this is our candidate document set\n",
    "                if docid not in features[qid]:\n",
    "                    features[qid][docid] = {}\n",
    "            else:  # for other features, we ignore the doc if it's not in the candidate document set\n",
    "                if docid not in features[qid]:\n",
    "                    continue\n",
    "            features[qid][docid][fid] = doc.get(\"_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document features\n",
    "\n",
    "  - Feature #7: length of content field\n",
    "  - Feature #8: length of title field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing document features for query # 443\n",
      "Computing document features for query # 404\n",
      "Computing document features for query # 622\n",
      "Computing document features for query # 436\n",
      "Computing document features for query # 448\n",
      "Computing document features for query # 658\n",
      "Computing document features for query # 310\n",
      "Computing document features for query # 367\n",
      "Computing document features for query # 378\n",
      "Computing document features for query # 389\n",
      "Computing document features for query # 651\n",
      "Computing document features for query # 408\n",
      "Computing document features for query # 374\n",
      "Computing document features for query # 303\n",
      "Computing document features for query # 393\n",
      "Computing document features for query # 354\n",
      "Computing document features for query # 330\n",
      "Computing document features for query # 325\n",
      "Computing document features for query # 648\n",
      "Computing document features for query # 322\n",
      "Computing document features for query # 433\n",
      "Computing document features for query # 416\n",
      "Computing document features for query # 419\n",
      "Computing document features for query # 426\n",
      "Computing document features for query # 347\n",
      "Computing document features for query # 625\n",
      "Computing document features for query # 427\n",
      "Computing document features for query # 409\n",
      "Computing document features for query # 353\n",
      "Computing document features for query # 344\n",
      "Computing document features for query # 375\n",
      "Computing document features for query # 639\n",
      "Computing document features for query # 650\n",
      "Computing document features for query # 341\n",
      "Computing document features for query # 439\n",
      "Computing document features for query # 638\n",
      "Computing document features for query # 362\n",
      "Computing document features for query # 399\n",
      "Computing document features for query # 435\n",
      "Computing document features for query # 397\n",
      "Computing document features for query # 689\n",
      "Computing document features for query # 314\n",
      "Computing document features for query # 401\n",
      "Computing document features for query # 383\n",
      "Computing document features for query # 336\n",
      "Computing document features for query # 363\n",
      "Computing document features for query # 345\n",
      "Computing document features for query # 372\n",
      "Computing document features for query # 394\n",
      "Computing document features for query # 307\n"
     ]
    }
   ],
   "source": [
    "for qid, query in queries.items():\n",
    "    print(\"Computing document features for query #\", qid)\n",
    "    for doc_id in features[qid].keys():\n",
    "        # get document term vector from elastic\n",
    "        tv = es.termvectors(index=INDEX_NAME, doc_type=DOC_TYPE, id=doc_id, fields=[\"title\", \"content\"],\n",
    "                              term_statistics=True).get(\"term_vectors\", {})\n",
    "\n",
    "        # content field length\n",
    "        len_content = sum([s[\"term_freq\"] for t, s in tv.get(\"content\", {}).get(\"terms\", {}).items()])\n",
    "        features[qid][doc_id][NUM_FEAT+1] = len_content\n",
    "        \n",
    "        # title field length\n",
    "        len_title = sum([s[\"term_freq\"] for t, s in tv.get(\"title\", {}).get(\"terms\", {}).items()])\n",
    "        features[qid][doc_id][NUM_FEAT+2] = len_title\n",
    "            \n",
    "NUM_FEAT += 2        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query features\n",
    "\n",
    "  - Feature #9: query length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing query features for query # 443\n",
      "Computing query features for query # 404\n",
      "Computing query features for query # 622\n",
      "Computing query features for query # 436\n",
      "Computing query features for query # 448\n",
      "Computing query features for query # 658\n",
      "Computing query features for query # 310\n",
      "Computing query features for query # 367\n",
      "Computing query features for query # 378\n",
      "Computing query features for query # 389\n",
      "Computing query features for query # 651\n",
      "Computing query features for query # 408\n",
      "Computing query features for query # 374\n",
      "Computing query features for query # 303\n",
      "Computing query features for query # 393\n",
      "Computing query features for query # 354\n",
      "Computing query features for query # 330\n",
      "Computing query features for query # 325\n",
      "Computing query features for query # 648\n",
      "Computing query features for query # 322\n",
      "Computing query features for query # 433\n",
      "Computing query features for query # 416\n",
      "Computing query features for query # 419\n",
      "Computing query features for query # 426\n",
      "Computing query features for query # 347\n",
      "Computing query features for query # 625\n",
      "Computing query features for query # 427\n",
      "Computing query features for query # 409\n",
      "Computing query features for query # 353\n",
      "Computing query features for query # 344\n",
      "Computing query features for query # 375\n",
      "Computing query features for query # 639\n",
      "Computing query features for query # 650\n",
      "Computing query features for query # 341\n",
      "Computing query features for query # 439\n",
      "Computing query features for query # 638\n",
      "Computing query features for query # 362\n",
      "Computing query features for query # 399\n",
      "Computing query features for query # 435\n",
      "Computing query features for query # 397\n",
      "Computing query features for query # 689\n",
      "Computing query features for query # 314\n",
      "Computing query features for query # 401\n",
      "Computing query features for query # 383\n",
      "Computing query features for query # 336\n",
      "Computing query features for query # 363\n",
      "Computing query features for query # 345\n",
      "Computing query features for query # 372\n",
      "Computing query features for query # 394\n",
      "Computing query features for query # 307\n"
     ]
    }
   ],
   "source": [
    "for qid, query in queries.items():\n",
    "    print(\"Computing query features for query #\", qid)\n",
    "    len_query = len(query.split())\n",
    "    for doc_id in features[qid].keys():\n",
    "        features[qid][doc_id][NUM_FEAT+1] = len_query\n",
    "            \n",
    "NUM_FEAT += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating training data and writing it to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** For IR tasks, there should be no \"missing\" features. When comparing a document field against a query, there is always a retrieval score (which may be 0).  To keep this exercise simple, we use this trick of getting top-1000 docs for each method/field combination from Elasticsearch and using those retrieval scores. If the doc was not returned in the top-1000, then we'll take the retrieval score for that field to be 0. \n",
    "\n",
    "In Assignment-3, you should score each of document fields by computing the retrieval scores based on termvector information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(FEATURES_FILE, \"w\") as fout:\n",
    "    for qid, query in queries.items():\n",
    "        for docid, ft in features[qid].items():\n",
    "            # Note that docid will not have a feature value for feature ID i\n",
    "            # if it was not retrieved in the top-1000 positions for that feature\n",
    "            # Then we use 0 as retrieval score\n",
    "            for fid in range(1, len(ES_CONFIG) + 1):\n",
    "                if fid not in ft:\n",
    "                    ft[fid] = 0\n",
    "            \n",
    "            # min-max normalization: this is only to be applied to features that\n",
    "            # are not compatible (i.e., comparable) across queries\n",
    "            # document and query lengths are comparable => no normalization needed\n",
    "            # document retrieval scores depend on the query length (i.e., not comparable) => normalization needed\n",
    "            for fid in range(1, len(ES_CONFIG) + 1):  # normalize first 6 features\n",
    "                minmax_norm(features, fid)\n",
    "            \n",
    "            # relevance label is determined based on the ground truth (qrels) file\n",
    "            label = 1 if docid in qrels.get(qid, []) else 0\n",
    "                        \n",
    "            feat_str = ['{}:{}'.format(k,v) for k,v in ft.items()]\n",
    "            fout.write(\" \".join([str(label), qid, docid] + feat_str) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Loading training data from file and performing retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning-to-rank code copy-pasted from the example (Task 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A class for pointwise-based learning to rank model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        :param classifier: an instance of scikit-learn regressor\n",
    "        \"\"\"\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains and LTR model.\n",
    "        :param X: features of training instances\n",
    "        :param y: relevance assessments of training instances\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.regressor is not None\n",
    "        self.model = self.regressor.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_ids):\n",
    "        \"\"\"\n",
    "        Predicts relevance labels and rank documents for a given query\n",
    "        :param ft: a list of features for query-doc pairs\n",
    "        :param ft: a list of document ids\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(ft)\n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "\n",
    "        results = []\n",
    "        for i in sort_indices:\n",
    "            results.append((doc_ids[i], rel_labels[i]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data_from_file(path):\n",
    "    \"\"\"\n",
    "    :param path: path of file\n",
    "    :return: X features of data, y labels of data, group a list of numbers indicate how many instances for each query\n",
    "    \"\"\"\n",
    "    X, y, qids, doc_ids = [], [], [], []\n",
    "    with open(path, \"r\") as f:\n",
    "        i, s_qid = 0, None\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            label = int(items[0])\n",
    "            qid = items[1]\n",
    "            doc_id = items[2]\n",
    "            features = np.array([float(i.split(\":\")[1]) for i in items[3:]])\n",
    "            # replace -1 values with np.nan\n",
    "            for j in range(len(features)):\n",
    "                if features[j] == -1:\n",
    "                    features[j] = 0\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "            qids.append(qid)\n",
    "            doc_ids.append(doc_id)\n",
    "\n",
    "    return X, y, qids, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, applying LTR for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#queries:  50\n",
      "#query-doc pairs:  5000\n"
     ]
    }
   ],
   "source": [
    "X, y, qids, doc_ids = read_data_from_file(path=FEATURES_FILE)\n",
    "qids_unique= list(set(qids))\n",
    "\n",
    "print(\"#queries: \", len(qids_unique))\n",
    "print(\"#query-doc pairs: \", len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 443\n",
      "\t\tRanking docs for queryID 648\n",
      "\t\tRanking docs for queryID 639\n",
      "\t\tRanking docs for queryID 651\n",
      "\t\tRanking docs for queryID 336\n",
      "\t\tRanking docs for queryID 419\n",
      "\t\tRanking docs for queryID 344\n",
      "\t\tRanking docs for queryID 433\n",
      "\t\tRanking docs for queryID 409\n",
      "\t\tRanking docs for queryID 399\n",
      "Fold #2\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 404\n",
      "\t\tRanking docs for queryID 435\n",
      "\t\tRanking docs for queryID 625\n",
      "\t\tRanking docs for queryID 310\n",
      "\t\tRanking docs for queryID 363\n",
      "\t\tRanking docs for queryID 439\n",
      "\t\tRanking docs for queryID 347\n",
      "\t\tRanking docs for queryID 341\n",
      "\t\tRanking docs for queryID 397\n",
      "\t\tRanking docs for queryID 427\n",
      "Fold #3\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 622\n",
      "\t\tRanking docs for queryID 367\n",
      "\t\tRanking docs for queryID 314\n",
      "\t\tRanking docs for queryID 345\n",
      "\t\tRanking docs for queryID 408\n",
      "\t\tRanking docs for queryID 353\n",
      "\t\tRanking docs for queryID 638\n",
      "\t\tRanking docs for queryID 394\n",
      "\t\tRanking docs for queryID 354\n",
      "\t\tRanking docs for queryID 307\n",
      "Fold #4\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 436\n",
      "\t\tRanking docs for queryID 393\n",
      "\t\tRanking docs for queryID 322\n",
      "\t\tRanking docs for queryID 378\n",
      "\t\tRanking docs for queryID 372\n",
      "\t\tRanking docs for queryID 401\n",
      "\t\tRanking docs for queryID 650\n",
      "\t\tRanking docs for queryID 374\n",
      "\t\tRanking docs for queryID 325\n",
      "\t\tRanking docs for queryID 362\n",
      "Fold #5\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 375\n",
      "\t\tRanking docs for queryID 448\n",
      "\t\tRanking docs for queryID 389\n",
      "\t\tRanking docs for queryID 416\n",
      "\t\tRanking docs for queryID 383\n",
      "\t\tRanking docs for queryID 689\n",
      "\t\tRanking docs for queryID 303\n",
      "\t\tRanking docs for queryID 658\n",
      "\t\tRanking docs for queryID 330\n",
      "\t\tRanking docs for queryID 426\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "\n",
    "fout = open(OUTPUT_FILE, \"w\")\n",
    "# write header\n",
    "fout.write(\"QueryId,DocumentId\\n\")\n",
    "    \n",
    "for f in range(FOLDS):\n",
    "    print(\"Fold #{}\".format(f + 1))\n",
    "    \n",
    "    train_qids, test_qids = [], []  # holds the IDs of train and test queries\n",
    "    train_ids, test_ids = [], []  # holds the instance IDs (indices in X )\n",
    "\n",
    "    for i in range(len(qids_unique)):\n",
    "        qid = qids_unique[i]\n",
    "        if i % FOLDS == f:  # test query\n",
    "            test_qids.append(qid)\n",
    "        else:  # train query\n",
    "            train_qids.append(qid)\n",
    "\n",
    "    train_X, train_y = [], []  # training feature values and target labels\n",
    "    test_X = []  # for testing we only have feature values\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if qids[i] in train_qids:\n",
    "            train_X.append(X[i])\n",
    "            train_y.append(y[i])\n",
    "        else:\n",
    "            test_X.append(X[i])\n",
    "\n",
    "    # Create and train LTR model\n",
    "    print(\"\\tTraining model ...\")\n",
    "    clf = RandomForestRegressor(max_features=4, random_state=0) \n",
    "    ltr = PointWiseLTRModel(clf)\n",
    "    ltr._train(train_X, train_y)\n",
    "    \n",
    "    # Apply LTR model on the remaining fold (test queries)\n",
    "    print(\"\\tApplying model ...\")\n",
    "    \n",
    "    for qid in set(test_qids):\n",
    "        print(\"\\t\\tRanking docs for queryID {}\".format(qid))\n",
    "        # Collect the features and docids for that (test) query `qid`\n",
    "        test_ft, test_docids = [], []\n",
    "        for i in range(len(X)):\n",
    "            if qids[i] == qid:\n",
    "                test_ft.append(X[i])\n",
    "                test_docids.append(doc_ids[i])\n",
    "        \n",
    "        # Get ranking\n",
    "        r = ltr.rank(test_ft, test_docids)    \n",
    "        # Write the results to file\n",
    "        for doc, score in r:\n",
    "            fout.write(qid + \",\" + doc + \"\\n\")\n",
    "        \n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
